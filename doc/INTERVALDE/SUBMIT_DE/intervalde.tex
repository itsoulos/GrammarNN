%% LyX 2.3.7 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[journal,article,submit,pdftex,moreauthors]{Definitions/mdpi}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{url}
\usepackage{amsmath}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.

\Title{Introducing an evolutionary method to create the bounds of artificial
neural networks}

\TitleCitation{Introducing an evolutionary method to create the bounds of artificial
neural networks}

\Author{Ioannis G. Tsoulos$^{1,*}$, Vasileios Charilogis$^{2}$, Dimitrios
Tsalikakis$^{3}$}

\AuthorNames{Ioannis G. Tsoulos, Vasileios Charilogis, Dimitrios Tsalikakis}

\AuthorCitation{Tsoulos, I.G.; Charilogis, V.; Tsalikakis D.}


\address{$^{1}$\quad{}Department of Informatics and Telecommunications,
University of Ioannina, Greece; itsoulos@uoi.gr\\
$^{2}$\quad{}Department of Informatics and Telecommunications, University
of Ioannina, Greece; v.charilog@uoi.gr\\
$^{3}\quad$Department of Engineering Informatics and Telecommunications,
University of Western Macedonia, 50100 Kozani, Greece; tsalikakis@gmail.com}


\corres{Correspondence: itsoulos@uoi.gr}


\abstract{Artificial neural networks are widely used in applications from various
scientific fields and in a multitude of practical applications. In
recent years, a multitude of scientific publications have been presented
on the effective training of their parameters, but in many cases overfitting
problems appear, where the artificial neural network shows poor results
when used on data that was not present during training. This text
proposes the incorporation of a three - stage evolutionary technique,
which has its roots in the differential evolution technique, for the
effective training of the parameters of artificial neural networks
and the avoidance of the problem of overfitting. The new method effectively
constructs the parameter value range of the artificial neural network,
achieving both a reduction in training error and preventing the network
from experiencing overfitting phenomena. This new technique was successfully
applied to a wide range of problems from the relevant literature and
the results were extremely promising. }


\keyword{Neural networks; Evolutionary algorithms; Stochastic methods; Differential
Evolution}

\DeclareTextSymbolDefault{\textquotedbl}{T1}
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================


% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal and change "submit" to "accept". The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% acoustics, actuators, addictions, admsci, adolescents, aerospace, agriculture, agriengineering, agronomy, ai, algorithms, allergies, alloys, analytica, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biotech, birds, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinpract, clockssleep, cmd, coasts, coatings, colloids, colorants, commodities, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryptography, crystals, csmf, ctn, curroncol, currophthalmol, cyber, dairy, data, dentistry, dermato, dermatopathology, designs, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecologies, econometrics, economies, education, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, environsciproc, epidemiologia, epigenomes, est, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, foundations, fractalfract, fuels, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gels, genealogy, genes, geographies, geohazards, geomatics, geosciences, geotechnics, geriatrics, hazardousmatters, healthcare, hearts, hemato, heritage, highthroughput, histories, horticulturae, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, ijerph, ijfs, ijgi, ijms, ijns, ijtm, ijtpp, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmmp, jmp, jmse, jne, jnt, jof, joitmc, jor, journalmedia, jox, jpm, jrfm, jsan, jtaer, jzbg, kidney, kidneydial, knowledge, land, languages, laws, life, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrology, micro, microarrays, microbiolres, micromachines, microorganisms, microplastics, minerals, mining, modelling, molbank, molecules, mps, msf, mti, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nri, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, pollutants, polymers, polysaccharides, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, publications, quantumrep, quaternary, qubs, radiation, reactions, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, ruminants, safety, sci, scipharm, seeds, sensors, separations, sexes, signals, sinusitis, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, sports, standards, stats, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, taxonomy, technologies, telecom, test, textiles, thalassrep, thermo, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, viruses, vision, waste, water, wem, wevj, wind, women, world, youth, zoonoticdis 

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, book, bookreview, briefreport, casereport, comment, commentary, communication, conferenceproceedings, correction, conferencereport, entry, expressionofconcern, extendedabstract, datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, systematicreview, supfile, technicalnote, viewpoint, guidelines, registeredreport, tutorial
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figures are used, remove the option pdftex and use LaTeX and dvi2pdf.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
 
\setcounter{page}{\@firstpage} 

\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2024}
\copyrightyear{2024}
%\externaleditor{Academic Editor: Firstname Lastname} % For journal Automation, please change Academic Editor to "Communicated by"
\datereceived{}
\daterevised{ } % Comment out if no revised date
\dateaccepted{}
\datepublished{}
%\datecorrected{} % Corrected papers include a "Corrected: XXX" date in the original paper.
%\dateretracted{} % Corrected papers include a "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, cleveref, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences:
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data:
%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}

%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{Instead of the abstract}
%\entrylink{The Link to this entry published on the encyclopedia platform.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Advances in Respiratory Medicine
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%

%\noindent This is an obligatory section in â€œAdvances in Respiratory Medicineâ€, whose goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2~bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What is the implication of the main finding?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makeatother

\begin{document}
\maketitle

\section{Introduction}

One of the most widespread machine learning models with many applications
is artificial neural networks \citep{nn1,nn2}. Artificial neural
networks are parametric models defined as $N\left(\overrightarrow{x},\overrightarrow{w}\right)$,
where the vector $\overrightarrow{x}$ represents the input pattern
for the neural network and the vector $\overrightarrow{w}$ denotes
the associated set of parameters that should be estimated by some
optimization procedure. These optimization procedures usually minimize
the so - called training error, defined as:
\begin{equation}
E\left(N\left(\overrightarrow{x},\overrightarrow{w}\right)\right)=\sum_{i=1}^{M}\left(N\left(\overrightarrow{x}_{i},\overrightarrow{w}\right)-y_{i}\right)^{2}\label{eq:eq1}
\end{equation}
The set $\left(\overrightarrow{x_{i}},y_{i}\right),\ i=1,...,M$ stands
for the training set of the dataset and the values $y_{i}$ are the
expected outputs for each pattern $\overrightarrow{x_{i}}$. These
models have been applied on wide series of applications from the related
literature, such as\textbf{ }image processing \citep{nn_image}, time
series forecasting \citep{nn_timeseries}, credit card analysis \citep{nn_credit},
physics problems \citep{nnphysics1,nnphysics2},\textbf{ }solar radiation
prediction \citep{nn_solar}, agriculture problems \citep{nnagr2}
etc.

The equation \ref{eq:eq1} has been tackled by a variety of optimization
and metaheuristic methods, such as the Back Propagation algorithm
\citep{bpnn1,bpnn2}, the RPROP method \citep{rpropnn-1,rpropnn-2},
the Adam Optimizer \citep{nn_adam}, the Levenberg Marquardt method
\citep{nn_leve} etc. Furthermore, global optimization techniques
have been also applied to neural network training. Among them one
can detect the Simulated Annealing approach \citep{nn_siman}, the
Genetic Algorithm approach \citep{geneticnn}, the Particle Swarm
Optimization (PSO) method \citep{psonn}, the Ant Colony Optimization
procedure \citep{aco_nn}, the Gray Wolf Optimizer \citep{gwo_nn},
the Whale optimization technique \citep{whale_nn} etc. Also,\textbf{
}Sexton et al proposed the incorporation of tabu search algorithm
for neural network training \citep{tabunn}, Zhang et al suggested
a hybrid algorithm that combines the PSO method and the Back Propagation
algorithm for efficient neural networks training \citep{nn_hybrid}.
Recently,\textbf{ }Zhao et al suggested the usage of a new Cascaded
Forward Algorithm to train artificial neural networks \citep{nn_cascade}.
Also, since in recent years there has been an explosive development
and use of parallel computing architectures, several researchers have
proposed their use for the efficient and rapid training of artificial
neural networks \citep{nn_gpu1,nn_gpu2}.

However, although the above techniques are extremely effective in
reducing the training error of artificial neural networks, they often
cause the problem of overfitting, where the artificial neural network
exhibits poor results when applied to data that was not present during
its training. An overview of the overfitting problem can be found
in the work of Ying \citep{nn_overfitting}. A series of methods has
been proposed in the recent literature to tackle the overfitting problem,
such as the weight sharing method \citep{nnsharing1,nnsharing2},\textbf{
}pruning techniques \citep{nnprunning1,nnprunning2}, the incorporation
of early stopping \citep{nnearly1,nnearly2}, weight decaying \citep{nndecay1,nndecay2}
etc. Furthermore, it has been suggested by various researchers the
usage of dynamic architectures to handle the overfitting problem \citep{nn_arch1,nn_arch2,nn_arch3}

This work proposes the adoption of a three-stage technique that will
have two goals: the effective training of artificial neural networks
and the avoidance of the phenomenon of overfitting. In the first phase,
a genetic algorithm \citep{gen_review} is used to detect an initial
range of values for the parameters of the artificial neural network.
This genetic algorithm uses a modified version of the artificial neural
network's training error as a fitness function, in order to avoid
the problem of overfitting. During the second phase of the proposed
algorithm, a bounding technique which is based on the Differential
Evolution algorithm \citep{de_review} is used in order to efficiently
identify promising ranges for the parameters of the neural networks.
The Differential Evolution method was used at this stage of the proposed
procedure as it has only a small number of parameters that the user
must initialize, but also because it has been used with excellent
success in a wide range of practical optimization applications \citep{de_symmetry1,de_symmetry3,de_symmetry6,de_symmetry7}.
During the third phase, a genetic algorithm is applied to train efficiently
the neural network using the bounds that have been produced in the
second phase for the parameters of the model. The proposed method
have been applied on a wide series of classification and regression
datasets and comparison was performed against traditional techniques
for the training of neural networks.

The remaining of this article is organized as follows: in section
\ref{sec:Materials-and-Methods} the proposed method is discussed
in detail, in section \ref{sec:Results} the used datasets as well
as the conducted experiments are discussed and finally, in section
\ref{sec:Conclusions} some conclusions are presented.

\section{The proposed method\label{sec:Materials-and-Methods}}

In this section, a detailed presentation and analysis of the three
stages of the proposed technique for the effective training of artificial
neural networks is provided.

\subsection{The genetic algorithm of the first phase }

The neural network used in the current work is a network with one
processing level and it can be defined using the following equation:
\begin{equation}
N\left(\overrightarrow{x},\overrightarrow{w}\right)=\sum_{i=1}^{H}w_{(d+2)i-(d+1)}\sigma\left(\sum_{j=1}^{d}x_{j}w_{(d+2)i-(d+1)+j}+w_{(d+2)i}\right)\label{eq:nn}
\end{equation}
that have been proposed in \citep{nnc}. In this equation the constant
$H$ represents the number of processing units of the network and
the constant $d$ stands for the dimension of the input pattern $\overrightarrow{x}$.
Following the equation, one can derive that the total number of parameters
of the network is calculated as $n=\left(d+2\right)H$. The current
work adopts neural networks of one processing layer (hidden layer).
According to the the Hornik's theorem \citep{Hornik} these networks
can approximate any function with a sufficient number of computing
units in the hidden layer. The function$\ \sigma(x)$ denotes the
sigmoid function with the following formula: 

\begin{equation}
\sigma(x)=\frac{1}{1+\exp(-x)}\label{eq:sig}
\end{equation}
An example plot for this function is shown in Figure \ref{fig:examplePlot}.

\begin{figure}[H]
\begin{centering}
\includegraphics{sig}
\par\end{centering}
\caption{An example plot of the sigmoid function.\label{fig:examplePlot}}

\end{figure}
As is evident from this particular form, this function tends very
quickly to 1 as $x$ goes to infinity and very quickly to 0 as the
parameter $x$ gets negative values. This phenomenon has the effect
of the artificial neuronal network to lose its general abilities,
as large changes in the values of the parameters cause no substantial
change in the response of the sigmoid function. The function $B(a)$
is used here to measure this effect and it is calculated using the
algorithm \ref{alg:CalculationBound}.

\textbf{}
\begin{algorithm}[H]
\textbf{\caption{Calculating the quantity $B(N\left(\protect\overrightarrow{x},\protect\overrightarrow{w}\right),a)$
with $a>0$ for a a provided neural network $N(x,w).$\label{alg:CalculationBound}}
}
\begin{enumerate}
\item \textbf{Function} $B(N\left(\overrightarrow{x},\overrightarrow{w}\right),a)$
\item \textbf{Define $c=0$}
\item \textbf{For $i=1..H$ Do}
\begin{enumerate}
\item \textbf{For $j=1..M$ Do}
\begin{enumerate}
\item Set $v=\sum_{k=1}^{d}\left(w_{(d+2)i-(d+i)+k}x_{jk}\right)+w_{(d+2)i}$
\item \textbf{If $\left|v\right|>a$ then $c=c+1$}
\end{enumerate}
\item \textbf{EndFor}
\end{enumerate}
\item \textbf{EndFor}
\item \textbf{Return $\frac{c}{H\star M}$}
\item \textbf{End Function}
\end{enumerate}
\end{algorithm}
The function $B(N\left(\overrightarrow{x},\overrightarrow{w}\right),a)$
is used to calculate the fitness value for the used genetic algorithm
described subsequently:
\begin{enumerate}
\item \textbf{Initialization step}.
\begin{enumerate}
\item \textbf{Set} the number of chromosomes denoted as $N_{c}$ and the
maximum number of allowed generations $N_{g}$.
\item \textbf{Set} as $p_{s}$ the selection rate and as $p_{m}$ the mutation
rate.
\item \textbf{Initialize} randomly the $N_{c}$ chromosomes. Each chromosome
$g_{i},\ i=1,..,N_{c}$ represents a neural network $N\left(\overrightarrow{x},\overrightarrow{g_{i}}\right)$.
\item \textbf{Set} $k=0$, the generation counter.
\end{enumerate}
\item \textbf{Fitness calculation step}.
\begin{enumerate}
\item \textbf{For} each chromosome $g_{i},\ i=1,..,N_{c}$ \textbf{do}
\begin{enumerate}
\item \textbf{Set} $E\left(N\left(\overrightarrow{x},\overrightarrow{g_{i}}\right)\right)=\sum_{j=1}^{M}\left(N\left(\overrightarrow{x_{j}},\overrightarrow{g_{i}}\right)-y_{j}\right)^{2}$
\item \textbf{Set} $b_{i}=B(N\left(\overrightarrow{x},\overrightarrow{g_{i}}\right),a)$
\item \textbf{Set} $f_{i}=E\left(N\left(\overrightarrow{x},\overrightarrow{g_{i}}\right)\right)\times\left(1+\lambda b_{i}^{2}\right)$
as the fitness value of chromosome $g_{i}$. The value $\lambda$
has the property $\lambda>1$.
\end{enumerate}
\item \textbf{End For}
\end{enumerate}
\item \textbf{Genetic operations step}.
\begin{enumerate}
\item \textbf{Copy} the best $\left(1-p_{s}\right)\times N_{c}$ chromosomes
to the next generation. The remaining will be substituted by individuals
produced by crossover and mutation.
\item \textbf{Perform }the crossover procedure. For every pair of constructed
chromosomes $\left(\widetilde{z},\widetilde{w}\right)$ two chromosomes
are selected from the current population using tournament selection.
The new chromosomes are created through the process suggested by Kaelo
et al \citep{kaelo}.
\item \textbf{Perform} the mutation procedure. For every element of each
chromosome alter this element randomly with probability $p_{m}$.
\end{enumerate}
\item \textbf{Termination check step}.
\begin{enumerate}
\item \textbf{Set} $k=k+1$
\item \textbf{If} $k<N_{g}$ \textbf{goto} fitness calculation step.
\end{enumerate}
\item \textbf{Bound creation step}.
\begin{enumerate}
\item \textbf{Obtain} the best chromosome $g^{*}$
\item \textbf{Create} the vectors $L^{*}$and $R^{*}$ as: 
\[
\begin{array}{ccc}
L_{i}^{*} & = & -f\left|g_{i}^{*}\right|,\ i=1,\ldots,n\\
R_{i}^{*} & = & f\left|g_{i}^{*}\right|,\ i=1,\ldots,n
\end{array}
\]
where $f>1$
\end{enumerate}
\end{enumerate}

\subsection{The bounding technique of the second phase }

During the second phase of the proposed algorithm, a systematic attempt
is made to identify the optimal value interval within the vectors
$L^{*}$ and $R^{*}$ identified in the previous phase. For this reason,
an evolutionary technique that has its bases in the differential evolution
technique is applied here. In this phase, the agents that constitute
the candidate solutions generated by the differential evolution technique
constitute ranges of values defined as: $\left[\overrightarrow{L_{k}},\overrightarrow{R_{k}}\right]$.
Also, the fitness value for each agent is defined as an interval $f=\left[f_{1},f_{2}\right].$
In order to compare two intervals $a=\left[a_{1},a_{2}\right]$ and
$b=\left[b_{1},b_{2}\right]$ the comparison operator $D(a,b)$ is
used with the following definition:
\begin{equation}
D(a,b)=\begin{cases}
\mbox{TRUE}, & a_{1}<b_{1},\mbox{OR\ \ensuremath{\left(a_{1}=b_{1}\ \mbox{AND}\ a_{2}<b_{2}\right)}}\\
\mbox{FALSE}, & \mbox{\mbox{OTHERWISE}}
\end{cases}\label{eq:eqD}
\end{equation}
The steps of the procedure used in the second phase have as follows:
\begin{enumerate}
\item \textbf{Initialization step}.
\begin{enumerate}
\item \textbf{Set} the number of agents NP.
\item \textbf{Set} the crossover probability CR.
\item \textbf{Set} the maximum number of iterations $N_{k}$.
\item \textbf{Set} the number of samples $N_{s}$.
\item \textbf{Initialize} each agent $a_{i}=$$\left[\overrightarrow{L_{i}},\overrightarrow{R_{i}}\right],\ i=1,\ldots,\mbox{NP}$
randomly inside the vectors $L^{*}$and $R^{*}$ of the previous phase.
\item \textbf{Set} $k=0$ as the iteration counter.
\end{enumerate}
\item \textbf{Fitness calculation step.}
\begin{enumerate}
\item \textbf{For} $i=1,\ldots,\mbox{NP}$ \textbf{do}
\begin{enumerate}
\item \textbf{Calculate} the fitness $f_{i}$ of agent $a_{i}$ using the
algorithm \ref{alg:fitnessCalculation}.
\end{enumerate}
\item \textbf{End} \textbf{For}
\end{enumerate}
\item \textbf{Main step}.
\begin{enumerate}
\item \textbf{For} $i=1,\ldots,\mbox{NP}$ \textbf{do}
\begin{enumerate}
\item \textbf{Select} randomly three distinct agents $a_{r1},a_{r2},a_{r3}$.
\item \textbf{Select} randomly an integer value $R\in[0,n].$
\item Set $t=a_{i}$ as the trial point .
\item \textbf{For} $j=1,\ldots,n$ \textbf{do}
\begin{enumerate}
\item \textbf{If} $j=R$ OR $r\le\mbox{CR}$ set $t_{j}=a_{r1,j}+F_{r}\times\left(a_{r2,j}-a_{r3,j}\right)$
where $r$ and $F_{r}$ are random values in {[}0,1{]}.
\end{enumerate}
\item \textbf{End For}
\item \textbf{Set} $t_{f}$ as the fitness of the trial set of intervals
$t$. This fitness value is calculated using the algorithm \ref{alg:fitnessCalculation}.
\item \textbf{If} $d\left(t_{f},f_{i}\right)=\mbox{TRUE}$ \textbf{then}
$a_{i}=t$.
\end{enumerate}
\item \textbf{End For}
\end{enumerate}
\item \textbf{Termination check step}.
\begin{enumerate}
\item \textbf{Set} $k=k+1$
\item \textbf{If} $k\le N_{k}$ goto Main Step.
\end{enumerate}
\item \textbf{Final step}.
\begin{enumerate}
\item \textbf{Obtain} the best agent $a^{*}=\left[\overrightarrow{L_{a}^{*}},\overrightarrow{R_{a}^{*}}\right]$
\item \textbf{Return} $a^{*}$ as the best located interval.
\end{enumerate}
%
\end{enumerate}
\begin{algorithm}[H]

\caption{Fitness calculation for any agent $a=\left[\protect\overrightarrow{L_{a}},\protect\overrightarrow{R_{a}}\right]$.\label{alg:fitnessCalculation}}

\begin{enumerate}
\item \textbf{Take} $N_{s}$ random samples in $a$ and form the set $S_{a}=\left\{ \overrightarrow{s_{1}},\overrightarrow{s_{2},}\ldots,\overrightarrow{s_{N_{s}}}\right\} $.
\item \textbf{Set} $f_{\mbox{min}}=\infty$
\item \textbf{Set} $f_{\max}=-\infty$
\item \textbf{For} $i=1,\ldots,N_{s}$ \textbf{do}
\begin{enumerate}
\item \textbf{Calculate} $E_{i}=\sum_{j=1}^{M}\left(N\left(\overrightarrow{x_{j}},\overrightarrow{s_{i}}\right)-y_{j}\right)^{2}$
\item \textbf{If} $E_{i}<f_{\mbox{min}}$ set $f_{\mbox{min}}=E_{i}$
\item \textbf{If} $E_{i}>f_{\mbox{max}}$ set $f_{\mbox{max}}=E_{i}$
\end{enumerate}
\item \textbf{End For}
\item \textbf{Return} as fitness value the quantity $f_{a}=\left[f_{\mbox{min}},f_{\mbox{max}}\right]$
\end{enumerate}
\end{algorithm}


\subsection{The final training method}

In the last phase of the proposed procedure, a genetic algorithm is
applied to train the artificial neural network. The network is trained
within the interval $a^{*}$ identified in the second phase of the
procedure. The main steps of this genetic algorithm have as follows:
\begin{enumerate}
\item \textbf{Initialization step}.
\begin{enumerate}
\item \textbf{Set} as $N_{c}$ the number of chromosomes and as $N_{g}$
the maximum number of allowed generations.
\item \textbf{Set} as $p_{s}$ the selection rate and as $p_{m}$ the mutation
rate.
\item \textbf{Initialize} each chromosome $g_{i},\ i=1,\ldots,N_{c}$ inside
the bounds $a^{*}=\left[\overrightarrow{L_{a}^{*}},\overrightarrow{R_{a}^{*}}\right]$
of the second phase.
\item \textbf{Set} $k=0$ as the generation number.
\end{enumerate}
\item \textbf{Fitness calculation step}.
\begin{enumerate}
\item \textbf{For} $i=1,\ldots,N_{c}$ \textbf{do}
\begin{enumerate}
\item \textbf{Obtain} the neural network $N_{i}=N\left(\overrightarrow{x},\overrightarrow{g_{i}}\right)$
for each chromosome $\overrightarrow{g_{i}}$.
\item \textbf{Set} $f_{i}=\sum_{j=1}^{M}\left(N\left(\overrightarrow{x_{j}},\overrightarrow{g_{i}}\right)-y_{j}\right)^{2}$
as the associated fitness value.
\end{enumerate}
\item \textbf{End For}
\end{enumerate}
\item \textbf{Genetic operations step}.
\begin{enumerate}
\item \textbf{Perform} selection, crossover and mutation using the same
operations as in the first phase of the proposed method.
\end{enumerate}
\item \textbf{Termination check step}.
\begin{enumerate}
\item \textbf{Set} $k=k+1$
\item \textbf{If} $k\le N_{g}$ goto Fitness Calculation Step.
\end{enumerate}
\item \textbf{Testing step}.
\begin{enumerate}
\item \textbf{Obtain} the best chromosome $\overrightarrow{g^{*}}$. 
\item \textbf{Create} the corresponding neural network $N\left(\overrightarrow{x},\overrightarrow{g^{*}}\right)$
\item \textbf{Apply} this neural network to the associated test set and
report the results.
\end{enumerate}
\end{enumerate}

\section{Results\label{sec:Results}}

The proposed method was tested for its efficiency on a series of classification
and regression problems, that were obtained from the following online
databases:
\begin{enumerate}
\item The UCI database located in \url{https://archive.ics.uci.edu/}(accessed
on 17 February 2025)\citep{uci}
\item The Keel website, \url{https://sci2s.ugr.es/keel/datasets.php}(accessed
on 17 February 2025)\citep{Keel}.
\item The Statlib URL \url{ftp://lib.stat.cmu.edu/datasets/index.html }(accessed
on 17 February 2025). 
\end{enumerate}

\subsection{Datasets }

The following series of classification datasets was used in the conducted
experiments:
\begin{enumerate}
\item \textbf{Appendicitis}, which is medical dataset proposed in \citep{appendicitis}. 
\item \textbf{Alcohol}, which is related to experiments regarding alcohol
consumption \citep{alcohol}. 
\item \textbf{Australian}, which is related to various bank transactions\citep{australian}.
\item \textbf{Balance} dataset \citep{balance}, which is used in various
psychological experiments.
\item \textbf{Cleveland}, which is a medical dataset \citep{cleveland1,cleveland2}. 
\item \textbf{Circular} dataset, which is a dataset created artificially. 
\item \textbf{Dermatology}, a medical dataset with 6 classes which is related
to dermatology problems \citep{dermatology}.
\item \textbf{Ecoli}, which is used to problems regarding proteins \citep{ecoli}.
\item \textbf{Glass} dataset, which is related to glass component analysis. 
\item \textbf{Haberman}, a medical dataset for the detection of breast cancer. 
\item \textbf{Hayes-roth} dataset \citep{hayes-roth}, a dataset with 3
classes.
\item \textbf{Heart}, a medical dataset about heart diseases \citep{heart}
with two classes.
\item \textbf{HeartAttack}, a medical dataset used for the detection of
heart diseases.
\item \textbf{Housevotes}, that contains data about the Congressional voting
in USA \citep{housevotes}.
\item \textbf{Ionosphere}, that contains measurements about the ionosphere
\citep{ion1,ion2}.
\item \textbf{Liverdisorder}, which is a medical dataset \citep{liver,liver1}.
\item \textbf{Lymography }dataset \citep{lymography}.
\item \textbf{Mammographic}, a medical dataset related to breast cancer
\citep{mammographic}.
\item \textbf{Parkinsons}, a medical dataset related to Parkinson's disease
\citep{parkinsons1,parkinsons2}.
\item \textbf{Pima}, a medical dataset related to the presence of diabetes\citep{pima}.
\item \textbf{Phoneme}, used in sound experiments.
\item \textbf{Popfailures}, a dataset that containsclimate measurements
\citep{popfailures}.
\item \textbf{Regions2}, a medical dataset related to some liver biopsy
images \citep{regions2}.
\item \textbf{Saheart}, which is a medical dataset related to some heart
diseases.\citep{saheart}.
\item \textbf{Segment} dataset \citep{segment}, used in various image processing
cases.
\item \textbf{Statheart}, which is a medical dataset related to heart diseases.
\item \textbf{Spiral}, which is a dataset created artificially. 
\item \textbf{Student}, a dataset that contains measurements from experiments
conducted in schools \citep{student}.
\item \textbf{Transfusion}, which is a medical dataset \citep{transfusion}.
\item \textbf{Wdbc}, a medical dataset used to predict the presence of breast
cancer \citep{wdbc1,wdbc2}.
\item \textbf{Wine}, a dataset used to predict the quality of wines \citep{wine1,wine2}.
\item \textbf{EEG} dataset, which is a medical dataset related to EEG measurements\citep{eeg1,eeg2}
and the following cases were studied from this dataset: Z\_F\_S, ZO\_NF\_S
and ZONF\_S.
\item \textbf{Zoo}, a dataset used to predict the class of some animals
\citep{zoo} .
\end{enumerate}
Furthermore, the following list of regression datasets was incorporated
in the conducted experiments:
\begin{enumerate}
\item \textbf{Abalone}, a dataset related to the estimation of the age of
abalones \citep{abalone}.
\item \textbf{Airfoil}, a dataset provided by NASA \citep{airfoil}.
\item \textbf{Auto}, a dataset used for the estimation of fuel consumption
in cars.
\item \textbf{BK}, which is used in basketball games. 
\item \textbf{BL}, a dataset that contains measurements about electricity
experiments.
\item \textbf{Baseball}, a dataset that contains data used in the estimation
of the income of baseball players.
\item \textbf{Concrete}, a dataset used in civil engineering \citep{concrete}.
\item \textbf{DEE}, a dataset that contains 6 features, which is used for
the prediction of electricity prices.
\item \textbf{Friedman}, an artificial dataset\citep{friedman}.
\item \textbf{FY, }this dataset used to estimate the longevity of fruit
flies. 
\item \textbf{HO}, a dataset provided by the STATLIB repository with 13
features.
\item \textbf{Housing}, which is used for the prediction of house prices
\citep{housing}.
\item \textbf{Laser}, which is used in various laser experiments.
\item \textbf{LW}, which is a dataset with 9 features used to measure the
weight of babes.
\item \textbf{Mortgage}, which is a dataset with 15 features related to
the economy of USA.
\item \textbf{Plastic}, a dataset related to the pressure in plastics.
\item \textbf{PL}, a dataset provided by the STATLIB repository.
\item \textbf{Quake}, a dataset with 3 features that contains measurements
from earthquakes.
\item \textbf{SN}, a dataset with 11 features, which is used in experiments
related to trellising and pruning,.
\item \textbf{Stock}, a dataset with 9 features used to predict the prices
of various stocks.
\item \textbf{Treasury}, which is a dataset with 15 features used in economic
problems.
\end{enumerate}

\subsection{Experimental results}

The software used in the conducted experiments was coded in C++, using
the freely available Optimus programming tool, that can be downloaded
from \url{https://github.com/itsoulos/GlobalOptimus/}( accessed on
17 February 2025 ). Each experiment was conducted 30 times using different
seed for the random generator each time. For the validation of the
experimental results the ten - fold cross validation method was used.
The average classification error as measured on the corresponding
test set is reported for the classification datasets. This error is
calculated using the following formula:
\begin{equation}
E_{C}\left(N\left(\overrightarrow{x},\overrightarrow{w}\right)\right)=100\times\frac{\sum_{i=1}^{N}\left(\mbox{class}\left(N\left(\overrightarrow{x_{i}},\overrightarrow{w}\right)\right)-y_{i}\right)}{N}
\end{equation}
Also, the\textbf{ }average regression error is reported for the regression
datasets, that can be calculated as follows:
\begin{equation}
E_{R}\left(N\left(\overrightarrow{x},\overrightarrow{w}\right)\right)=\frac{\sum_{i=1}^{N}\left(N\left(\overrightarrow{x_{i}},\overrightarrow{w}\right)-y_{i}\right)^{2}}{N}
\end{equation}
All the experiments were performed on\textbf{ }an AMD Ryzen 5950X
with 128GB of RAM and the used operating system was Debian Linux.
The values used for the parameters of the proposed method are shown
in Table \ref{tab:settings}. 

\begin{table}[H]
\caption{The experimental settings used in the current algorithm.\label{tab:settings}}

\centering{}%
\begin{tabular}{|c|c|c|}
\hline 
PARAMETER & MEANING & VALUE\tabularnewline
\hline 
\hline 
$H$ & Processing nodes & 10\tabularnewline
\hline 
$N_{c}$ & Chromosomes & 500\tabularnewline
\hline 
$N_{g}$ & Generations & 200\tabularnewline
\hline 
$p_{s}$ & Selection rate & 0.9\tabularnewline
\hline 
$p_{m}$ & Mutation rate & 0.05\tabularnewline
\hline 
$f$ & Bounding value & 2.0\tabularnewline
\hline 
$\mbox{NP}$ & Agents & 200\tabularnewline
\hline 
$F$ & Differential weight & 0.8\tabularnewline
\hline 
$\mbox{CR}$ & Crossover probability & 0.9\tabularnewline
\hline 
$N_{s}$ & Number of samples & 50\tabularnewline
\hline 
$N_{k}$ & Iterations & 200\tabularnewline
\hline 
\end{tabular}
\end{table}
The following notation is used in the tables that contains the measurement
from the conducted experiments:
\begin{enumerate}
\item The column DATASET contains the name of the used dataset.
\item The column ADAM represents the experimental results from the application
of the ADAM optimization method \citep{nn_adam} to a neural networks
with $H=10$ processing nodes.
\item The column BFGS denotes the incorporation of the BFGS optimization
method \citep{powell} to train an artificial neural network with
$H=10$ processing nodes.
\item The column GENETIC represents the usage of a Genetic Algorithm with
the experimental settings of Table \ref{tab:settings}, used to train
a neural network with $H=10$ processing nodes.
\item The column PROPOSED denotes the proposed method.
\item The row AVERAGE is used to measure the average classification or regression
error for all dataset.
\end{enumerate}
The table \ref{tab:experClass} is used to provide the experimental
results for the classification datasets and the table \ref{tab:experRegression}
provides the corresponding results for the regression datasets.
\begin{table}[H]
\caption{Experimental results for the used classification datasets. The numbers
in cells represent average classification error as measured on the
corresponding test set.\label{tab:experClass}}

\centering{}%
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
DATASET & ADAM & BFGS & GENETIC & RBF & PROPOSED\tabularnewline
\hline 
\hline 
APPENDICITIS & 16.50\% & 18.00\% & 24.40\% & 12.23\% & 15.00\%\tabularnewline
\hline 
ALCOHOL & 57.78\% & 41.50\% & 39.57\% & 49.32\% & 18.33\%\tabularnewline
\hline 
AUSTRALIAN & 35.65\% & 38.13\% & 32.21\% & 34.89\% & 21.49\%\tabularnewline
\hline 
BALANCE & 12.27\% & 8.64\% & 8.97\% & 33.53\% & 7.79\%\tabularnewline
\hline 
CLEVELAND & 67.55\% & 77.55\% & 51.60\% & 67.10\% & 42.38\%\tabularnewline
\hline 
CIRCULAR & 19.95\% & 6.08\% & 5.99\% & 5.98\% & 6.50\%\tabularnewline
\hline 
DERMATOLOGY & 26.14\% & 52.92\% & 30.58\% & 62.34\% & 4.97\%\tabularnewline
\hline 
ECOLI & 64.43\% & 69.52\% & 54.67\% & 59.48\% & 40.30\%\tabularnewline
\hline 
GLASS & 61.38\% & 54.67\% & 52.86\% & 50.46\% & 54.38\%\tabularnewline
\hline 
HABERMAN & 29.00\% & 29.34\% & 28.66\% & 25.10\% & 26.53\%\tabularnewline
\hline 
HAYES-ROTH & 59.70\% & 37.33\% & 56.18\% & 64.36\% & 34.31\%\tabularnewline
\hline 
HEART & 38.53\% & 39.44\% & 28.34\% & 31.20\% & 13.11\%\tabularnewline
\hline 
HEARTATTACK & 45.55\% & 46.67\% & 29.03\% & 29.00\% & 21.90\%\tabularnewline
\hline 
HOUSEVOTES & 7.48\% & 7.13\% & 6.62\% & 6.13\% & 6.09\%\tabularnewline
\hline 
IONOSPHERE & 16.64\% & 15.29\% & 15.14\% & 16.22\% & 10.37\%\tabularnewline
\hline 
LIVERDISORDER & 41.53\% & 42.59\% & 31.11\% & 30.84\% & 29.94\%\tabularnewline
\hline 
LYMOGRAPHY & 39.79\% & 35.43\% & 28.42\% & 25.50\% & 17.93\%\tabularnewline
\hline 
MAMMOGRAPHIC & 46.25\% & 17.24\% & 19.88\% & 21.38\% & 16.63\%\tabularnewline
\hline 
PARKINSONS & 24.06\% & 27.58\% & 18.05\% & 17.41\% & 12.79\%\tabularnewline
\hline 
PHONEME & 29.43\% & 15.58\% & 15.55\% & 23.32\% & 18.10\%\tabularnewline
\hline 
PIMA & 34.85\% & 35.59\% & 32.19\% & 25.78\% & 25.03\%\tabularnewline
\hline 
POPFAILURES & 5.18\% & 5.24\% & 5.94\% & 7.04\% & 4.45\%\tabularnewline
\hline 
REGIONS2 & 29.85\% & 36.28\% & 29.39\% & 38.29\% & 25.19\%\tabularnewline
\hline 
SAHEART & 34.04\% & 37.48\% & 34.86\% & 32.19\% & 29.26\%\tabularnewline
\hline 
SEGMENT & 49.75\% & 68.97\% & 57.72\% & 59.68\% & 27.80\%\tabularnewline
\hline 
SONAR & 30.33\% & 25.85\% & 22.40\% & 27.90\% & 20.50\%\tabularnewline
\hline 
SPIRAL & 47.67\% & 47.99\% & 48.66\% & 44.87\% & 41.60\%\tabularnewline
\hline 
STATHEART & 44.04\% & 39.65\% & 27.25\% & 31.36\% & 19.74\%\tabularnewline
\hline 
STUDENT & 5.13\% & 7.14\% & 5.61\% & 5.49\% & 4.00\%\tabularnewline
\hline 
TRANSFUSION & 25.68\% & 25.84\% & 24.87\% & 26.41\% & 23.35\%\tabularnewline
\hline 
WDBC & 35.35\% & 29.91\% & 8.56\% & 7.27\% & 6.73\%\tabularnewline
\hline 
WINE & 29.40\% & 59.71\% & 19.20\% & 31.41\% & 6.29\%\tabularnewline
\hline 
Z\_F\_S & 47.81\% & 39.37\% & 10.73\% & 13.16\% & 8.38\%\tabularnewline
\hline 
ZO\_NF\_S & 47.43\% & 43.04\% & 21.54\% & 9.02\% & 4.32\%\tabularnewline
\hline 
ZONF\_S & 11.99\% & 15.62\% & 4.36\% & 4.03\% & 1.76\%\tabularnewline
\hline 
ZOO & 14.13\% & 10.70\% & 9.50\% & 21.93\% & 7.00\%\tabularnewline
\hline 
\textbf{AVERAGE} & \textbf{34.23\%} & \textbf{33.58\%} & \textbf{26.13\%} & \textbf{29.21\%} & \textbf{18.73\%}\tabularnewline
\hline 
\end{tabular}
\end{table}
\begin{table}[H]
\caption{Experimental results for the used regression datasets. Numbers in
cells represent average regression error as calculated on the corresponding
test set.\label{tab:experRegression}}

\centering{}%
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
DATASET & ADAM & BFGS & GENETIC & RBF & PROPOSED\tabularnewline
\hline 
\hline 
ABALONE & 4.30 & 5.69 & 7.17 & 7.37 & 4.32\tabularnewline
\hline 
AIRFOIL & 0.005 & 0.003 & 0.003 & 0.27 & 0.002\tabularnewline
\hline 
AUTO & 70.84 & 60.97 & 12.18 & 17.87 & 12.78\tabularnewline
\hline 
BK & 0.0252 & 0.28 & 0.027 & 0.02 & 0.02\tabularnewline
\hline 
BL & 0.622 & 2.55 & 5.74 & 0.013 & 0.006\tabularnewline
\hline 
BASEBALL & 77.90 & 119.63 & 103.60 & 93.02 & 60.74\tabularnewline
\hline 
CONCRETE & 0.078 & 0.066 & 0.0099 & 0.011 & 0.006\tabularnewline
\hline 
DEE & 0.63 & 2.36 & 1.013 & 0.17 & 0.19\tabularnewline
\hline 
FRIEDMAN & 22.90 & 1.263 & 1.249 & 7.23 & 2.21\tabularnewline
\hline 
FY & 0.038 & 0.19 & 0.65 & 0.041 & 0.067\tabularnewline
\hline 
HO & 0.035 & 0.62 & 2.78 & 0.03 & 0.015\tabularnewline
\hline 
HOUSING & 80.99 & 97.38 & 43.26 & 57.68 & 20.74\tabularnewline
\hline 
LASER & 0.03 & 0.015 & 0.59 & 0.03 & 0.004\tabularnewline
\hline 
LW & 0.028 & 2.98 & 1.90 & 0.03 & 0.011\tabularnewline
\hline 
MORTGAGE & 9.24 & 8.23 & 2.41 & 1.45 & 0.32\tabularnewline
\hline 
PL & 0.117 & 0.29 & 0.29 & 2.118 & 0.022\tabularnewline
\hline 
PLASTIC & 11.71 & 20.32 & 2.791 & 8.62 & 2.16\tabularnewline
\hline 
QUAKE & 0.07 & 0.42 & 0.04 & 0.07 & 0.036\tabularnewline
\hline 
SN & 0.026 & 0.40 & 2.95 & 0.027 & 0.023\tabularnewline
\hline 
STOCK & 180.89 & 302.43 & 3.88 & 12.23 & 5.57\tabularnewline
\hline 
TREASURY & 11.16 & 9.91 & 2.93 & 2.02 & 0.68\tabularnewline
\hline 
\textbf{AVERAGE} & \textbf{22.46} & \textbf{30.29} & \textbf{9.31} & \textbf{10.02} & \textbf{5.23}\tabularnewline
\hline 
\end{tabular}
\end{table}

The statistical analysis derived from Table \ref{tab:experClass}
presents the error rates for various classification datasets using
five different machine learning models: ADAM, BFGS, GENETIC, RBF,
and PROPOSED. The analysis focuses on comparing the models in terms
of their effectiveness, with a lower error rate indicating better
performance. Initially, it is observed that the PROPOSED model achieves
the lowest average error rate (18.73\%) compared to the other models,
which have average error rates of 34.23\% (ADAM), 33.58\% (BFGS),
26.13\% (GENETIC), and 29.21\% (RBF). This highlights the overall
superiority of the PROPOSED model. Examining individual datasets,
the PROPOSED model exhibits the lowest error rate in many cases, such
as \textquotedbl BALANCE\textquotedbl{} (7.79\%), \textquotedbl DERMATOLOGY\textquotedbl{}
(4.97\%), \textquotedbl STUDENT\textquotedbl{} (4.00\%), \textquotedbl ZO\_NF\_S\textquotedbl{}
(4.32\%), \textquotedbl ZONF\_S\textquotedbl{} (1.76\%), and \textquotedbl ZOO\textquotedbl{}
(7.00\%). Notably, its performance on \textquotedbl ZONF\_S,\textquotedbl{}
with an error rate of only 1.76\%, demonstrates exceptional accuracy.
In other datasets, such as \textquotedbl CIRCULAR\textquotedbl{}
and \textquotedbl HOUSEVOTES,\textquotedbl{} the PROPOSED model performs
very close to the best result but does not outperform the others.
In some cases, such as \textquotedbl SEGMENT\textquotedbl{} and \textquotedbl WINE,\textquotedbl{}
it shows higher error rates, indicating room for improvement with
specific types of data. In comparison, the ADAM model exhibits the
highest overall deviation, making it the least effective. The RBF
model also shows higher error rates compared to the PROPOSED model
in several datasets, such as \textquotedbl BALANCE\textquotedbl{}
and \textquotedbl ZOO.\textquotedbl{} Similarly, the BFGS model fails
to achieve low error rates in critical datasets such as \textquotedbl HEART\textquotedbl{}
and \textquotedbl STATHEART.\textquotedbl{} The GENETIC model demonstrates
intermediate performance, achieving lower error rates than ADAM and
BFGS but higher than the PROPOSED model. In conclusion, the analysis
confirms that the PROPOSED model is the most reliable and consistent
in most cases, achieving the lowest overall average error rate. 

The statistical analysis of Table \ref{tab:experRegression} examines
the absolute error values for various regression datasets using five
different machine learning models: ADAM, BFGS, GENETIC, RBF, and PROPOSED.
The analysis evaluates the models based on their performance, where
lower error values indicate better accuracy. The PROPOSED model achieves
the lowest average error (5.23) compared to the other models, which
have average errors of 22.46 (ADAM), 30.29 (BFGS), 9.31 (GENETIC),
and 10.02 (RBF). This result underscores the superior performance
of the PROPOSED model overall. Examining individual datasets, the
PROPOSED model consistently achieves the smallest error in several
cases. For instance, it outperforms other models in \textquotedbl AIRFOIL\textquotedbl{}
(0.002), \textquotedbl BL\textquotedbl{} (0.006), \textquotedbl CONCRETE\textquotedbl{}
(0.006), \textquotedbl HO\textquotedbl{} (0.015), \textquotedbl LASER\textquotedbl{}
(0.004), \textquotedbl LW\textquotedbl{} (0.011), \textquotedbl MORTGAGE\textquotedbl{}
(0.32), \textquotedbl PL\textquotedbl{} (0.022), and \textquotedbl PLASTIC\textquotedbl{}
(2.16). These results highlight the model's robustness across diverse
datasets. Notably, its performance on \textquotedbl AIRFOIL\textquotedbl{}
with an error of 0.002 and \textquotedbl LASER\textquotedbl{} with
0.004 demonstrate exceptional precision. In some datasets, such as
\textquotedbl SN\textquotedbl{} and \textquotedbl QUAKE,\textquotedbl{}
the PROPOSED model closely matches the best-performing results but
does not necessarily achieve the lowest error. In certain cases, like
\textquotedbl FRIEDMAN\textquotedbl{} and \textquotedbl STOCK,\textquotedbl{}
while still competitive, the model exhibits higher error values than
some of its competitors. Specifically, in \textquotedbl STOCK,\textquotedbl{}
the PROPOSED model records an error of 5.57, which is higher than
GENETIC's error of 3.88 but significantly lower than ADAM and BFGS.
These cases indicate areas where further optimization of the PROPOSED
model may be beneficial. Comparatively, the ADAM and BFGS models generally
exhibit higher error rates, with BFGS particularly underperforming
on datasets such as \textquotedbl BASEBALL\textquotedbl{} and \textquotedbl STOCK.\textquotedbl{}
The GENETIC model demonstrates strong performance, with its average
error being the second lowest (9.31). However, its results are still
consistently higher than the PROPOSED model on key datasets like \textquotedbl MORTGAGE\textquotedbl{}
and \textquotedbl HOUSING.\textquotedbl{} Similarly, the RBF model,
while competitive in some cases, such as \textquotedbl TREASURY,\textquotedbl{}
has higher errors in others, such as \textquotedbl AUTO\textquotedbl{}
and \textquotedbl BL.\textquotedbl{} In conclusion, the analysis
confirms that the PROPOSED model is the most effective and consistent
across the majority of regression datasets, achieving the lowest overall
average error. 

In Figure \ref{fig:statClass}, which pertains to classification datasets
for various machine learning models, the proposed method demonstrates
highly significant differences when compared to other methods. Specifically,
the p-values are as follows: PROPOSED vs ADAM: p=2.9e-11, PROPOSED
vs BFGS: p=1.3e-09, PROPOSED vs GENETIC: p=1.6e-08 and PROPOSED vs
RBF: p=8e-08. These results highlight the statistical significance
of the proposed method in classification tasks. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{img1}
\par\end{centering}
\caption{Statistical comparison of the used methods for the classification
datasets.\label{fig:statClass}}
\end{figure}

In Figure \ref{fig:statRegression}, focusing on regression datasets
for various machine learning models, the proposed method again shows
statistically significant differences when compared to other methods.
The corresponding p-values are: PROPOSED vs ADAM: p=0.0001, PROPOSED
vs BFGS: p=4.1e-05, PROPOSED vs GENETIC: p=0.0022 and PROPOSED vs
RBF: p=0.00073. These results indicate the superiority of the proposed
method across regression datasets with notable levels of significance. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{img2}
\par\end{centering}
\caption{Statistical comparison between the used methods for the regression
datasets.\label{fig:statRegression}}

\end{figure}


\subsubsection{Using different weight methods }

An additional experiment was executed where the parameter $F$ of
the differential evolution was altered using some well - known approaches
from the relevant literature. In the following tables the following
methods, as denoted in the experimental tables, were obtained for
the calculation of the parameter $F$:
\begin{enumerate}
\item RANDOM. This method is used for the current approach, where the differential
weight is a random number in $[0,1]$.
\item FIXED. This method is used when the value for $F$ is used, as denoted
in Table \ref{tab:settings}.
\item ADAPT. This method is used for the adaptive calculation of parameter
$F$ as proposed in \citep{de_adaptive}.
\item MIGRANT. For this case the calculation for parameter $F$ as proposed
in \citep{de_migrant} is adopted.
\end{enumerate}
The experimental results using the current method and the previously
mentioned method for differential weight are listed in Tables \ref{tab:experFClass}
and \ref{tab:experFRegression} for the classification datasets and
the regression datasets respectively.

\begin{table}[H]
\caption{Experimental results for the classification datasets and the proposed
method using a series of differential weight mechanisms.\label{tab:experFClass}}

\centering{}%
\begin{tabular}{|c|c|c|c|c|}
\hline 
DATASET & RANDOM & FIXED & ADAPT & MIGRANT\tabularnewline
\hline 
\hline 
APPENDICITIS & 15.00\% & 14.90\% & 15.30\% & 15.80\%\tabularnewline
\hline 
ALCOHOL & 18.33\% & 20.81\% & 22.08\% & 17.26\%\tabularnewline
\hline 
AUSTRALIAN & 21.49\% & 19.62\% & 23.91\% & 29.49\%\tabularnewline
\hline 
BALANCE & 7.79\% & 7.56\% & 8.39\% & 8.39\%\tabularnewline
\hline 
CLEVELAND & 42.38\% & 42.31\% & 41.79\% & 44.79\%\tabularnewline
\hline 
CIRCULAR & 6.50\% & 7.24\% & 7.40\% & 4.74\%\tabularnewline
\hline 
DERMATOLOGY & 4.97\% & 5.72\% & 5.71\% & 11.40\%\tabularnewline
\hline 
ECOLI & 40.30\% & 42.18\% & 45.51\% & 47.61\%\tabularnewline
\hline 
GLASS & 54.38\% & 54.33\% & 56.29\% & 49.00\%\tabularnewline
\hline 
HABERMAN & 26.53\% & 27.93\% & 30.00\% & 26.60\%\tabularnewline
\hline 
HAYES-ROTH & 34.31\% & 34.00\% & 28.62\% & 35.00\%\tabularnewline
\hline 
HEART & 13.11\% & 15.30\% & 14.56\% & 19.04\%\tabularnewline
\hline 
HEARTATTACK & 21.90\% & 19.57\% & 20.00\% & 20.70\%\tabularnewline
\hline 
HOUSEVOTES & 6.09\% & 6.00\% & 5.22\% & 5.48\%\tabularnewline
\hline 
IONOSPHERE & 10.37\% & 9.71\% & 8.86\% & 11.80\%\tabularnewline
\hline 
LIVERDISORDER & 29.94\% & 30.97\% & 29.56\% & 31.12\%\tabularnewline
\hline 
LYMOGRAPHY & 17.93\% & 18.93\% & 20.72\% & 20.93\%\tabularnewline
\hline 
MAMMOGRAPHIC & 16.63\% & 15.74\% & 16.12\% & 17.50\%\tabularnewline
\hline 
PARKINSONS & 12.79\% & 9.69\% & 11.58\% & 14.42\%\tabularnewline
\hline 
PHONEME & 18.10\% & 17.23\% & 18.15\% & 18.40\%\tabularnewline
\hline 
PIMA & 25.03\% & 27.66\% & 27.89\% & 27.70\%\tabularnewline
\hline 
POPFAILURES & 4.45\% & 5.19\% & 4.82\% & 4.39\%\tabularnewline
\hline 
REGIONS2 & 25.19\% & 25.03\% & 23.24\% & 30.92\%\tabularnewline
\hline 
SAHEART & 29.26\% & 32.26\% & 30.76\% & 32.28\%\tabularnewline
\hline 
SEGMENT & 27.80\% & 36.29\% & 33.83\% & 42.80\%\tabularnewline
\hline 
SONAR & 20.50\% & 20.85\% & 21.00\% & 22.10\%\tabularnewline
\hline 
SPIRAL & 41.60\% & 43.93\% & 45.67\% & 44.04\%\tabularnewline
\hline 
STATHEART & 19.74\% & 18.52\% & 16.67\% & 17.41\%\tabularnewline
\hline 
STUDENT & 4.00\% & 4.38\% & 4.25\% & 4.63\%\tabularnewline
\hline 
TRANSFUSION & 23.35\% & 22.95\% & 24.58\% & 23.50\%\tabularnewline
\hline 
WDBC & 6.73\% & 6.48\% & 7.50\% & 4.18\%\tabularnewline
\hline 
WINE & 6.29\% & 5.82\% & 6.35\% & 11.59\%\tabularnewline
\hline 
Z\_F\_S & 8.38\% & 7.00\% & 9.27\% & 10.43\%\tabularnewline
\hline 
ZO\_NF\_S & 4.32\% & 5.52\% & 6.00\% & 8.36\%\tabularnewline
\hline 
ZONF\_S & 1.76\% & 1.92\% & 2.20\% & 2.32\%\tabularnewline
\hline 
ZOO & 7.00\% & 4.90\% & 9.00\% & 6.10\%\tabularnewline
\hline 
\textbf{AVERAGE} & \textbf{18.73\%} & \textbf{19.12\%} & \textbf{19.52\%} & \textbf{20.62\%}\tabularnewline
\hline 
\end{tabular}
\end{table}
\begin{table}[H]
\caption{Experimental results for the regression datasets and the proposed
method using a variety of techniques for the calculation of differential
weight.\label{tab:experFRegression}}

\centering{}%
\begin{tabular}{|c|c|c|c|c|}
\hline 
DATASET & RANDOM & FIXED & ADAPT & MIGRANT\tabularnewline
\hline 
\hline 
ABALONE & 4.32 & 4.47 & 4.35 & 4.41\tabularnewline
\hline 
AIRFOIL & 0.002 & 0.003 & 0.003 & 0.003\tabularnewline
\hline 
AUTO & 12.78 & 13.58 & 13.98 & 11.46\tabularnewline
\hline 
BK & 0.02 & 0.057 & 0.021 & 0.021\tabularnewline
\hline 
BL & 0.006 & 0.008 & 0.006 & 0.008\tabularnewline
\hline 
BASEBALL & 60.74 & 65.12 & 67.71 & 60.10\tabularnewline
\hline 
CONCRETE & 0.006 & 0.026 & 0.007 & 0.004\tabularnewline
\hline 
DEE & 0.19 & 0.21 & 0.20 & 0.24\tabularnewline
\hline 
FRIEDMAN & 2.21 & 2.79 & 3.18 & 1.88\tabularnewline
\hline 
FY & 0.067 & 0.046 & 0.042 & 0.052\tabularnewline
\hline 
HO & 0.015 & 0.017 & 0.052 & 0.013\tabularnewline
\hline 
HOUSING & 20.74 & 24.63 & 24.83 & 24.71\tabularnewline
\hline 
LASER & 0.004 & 0.004 & 0.004 & 0.003\tabularnewline
\hline 
LW & 0.011 & 0.014 & 0.011 & 0.013\tabularnewline
\hline 
MORTGAGE & 0.32 & 0.71 & 0.32 & 0.22\tabularnewline
\hline 
PL & 0.022 & 0.022 & 0.022 & 0.023\tabularnewline
\hline 
PLASTIC & 2.16 & 2.15 & 2.15 & 2.36\tabularnewline
\hline 
QUAKE & 0.036 & 0.052 & 0.036 & 0.044\tabularnewline
\hline 
SN & 0.023 & 0.036 & 0.025 & 0.024\tabularnewline
\hline 
STOCK & 5.57 & 5.89 & 4.72 & 4.41\tabularnewline
\hline 
TREASURY & 0.68 & 0.64 & 0.51 & 0.65\tabularnewline
\hline 
\textbf{AVERAGE} & \textbf{5.23} & \textbf{5.74} & \textbf{5.82} & \textbf{5.27}\tabularnewline
\hline 
\end{tabular}
\end{table}

The statistical analysis of Table \ref{tab:experFClass} examines
the percentage error rates across various classification datasets
using four different computations of the critical weight differential
parameter for the proposed machine learning model: RANDOM, FIXED,
ADAPT, and MIGRANT. The RANDOM computation exhibits the lowest overall
average error rate (18.73\%), indicating superior performance compared
to the other computations: FIXED (19.12\%), ADAPT (19.52\%), and MIGRANT
(20.62\%). This suggests that the RANDOM computation is the most reliable
overall. Analyzing individual datasets, the RANDOM computation achieves
the best error rates in several cases. For instance, it records the
lowest error rates for datasets such as \textquotedbl ALCOHOL\textquotedbl{}
(18.33\%), \textquotedbl BALANCE\textquotedbl{} (7.79\%), \textquotedbl CIRCULAR\textquotedbl{}
(6.50\%), \textquotedbl HOUSEVOTES\textquotedbl{} (6.09\%), \textquotedbl PARKINSONS\textquotedbl{}
(12.79\%), \textquotedbl POPFAILURES\textquotedbl{} (4.45\%), and
\textquotedbl WDBC\textquotedbl{} (6.73\%). These results highlight
its effectiveness across a wide range of datasets. Specifically, the
4.45\% error rate for \textquotedbl POPFAILURES\textquotedbl{} stands
out as one of the lowest overall. In certain datasets, the FIXED computation
outperforms others, such as in \textquotedbl Z\_F\_S\textquotedbl{}
(7.00\%) and \textquotedbl ZOO\textquotedbl{} (4.90\%). However,
the difference from RANDOM is minimal. The MIGRANT computation demonstrates
the lowest error rates in only a few cases, such as \textquotedbl CIRCULAR\textquotedbl{}
(4.74\%) and \textquotedbl WDBC\textquotedbl{} (4.18\%), suggesting
it may be particularly effective for specific datasets. Meanwhile,
the ADAPT computation achieves lower error rates in a few scenarios
but generally remains less competitive. In other datasets, such as
\textquotedbl GLASS,\textquotedbl{} \textquotedbl SPIRAL,\textquotedbl{}
and \textquotedbl SEGMENT,\textquotedbl{} all computations show high
error rates, indicating these datasets are challenging to classify
regardless of the computation. Nevertheless, the RANDOM computation
remains consistently competitive even with these difficult datasets,
as observed in \textquotedbl STATHEART\textquotedbl{} and \textquotedbl ZONF\_S.\textquotedbl{}
In conclusion, the analysis reveals that the RANDOM computation is
the most effective overall, achieving the lowest average error rate
and demonstrating superior performance across a broad range of datasets.
However, there are instances where other computations, such as FIXED
and MIGRANT, show specialized advantages.

The statistical analysis of Table \ref{tab:experFRegression} pertains
to regression datasets, using four different calculations of the critical
parameter of differential weighting for the proposed machine learning
model: RANDOM, FIXED, ADAPT, and MIGRANT. The RANDOM calculation exhibits
the lowest average error (5.23), making it the most efficient overall
compared to FIXED (5.74), ADAPT (5.82), and MIGRANT (5.27). The small
difference between RANDOM and MIGRANT suggests comparable performance
between these two approaches, with RANDOM maintaining a slight edge.
For individual datasets, the RANDOM calculation achieves the lowest
error values in several cases, such as the \textquotedbl AUTO\textquotedbl{}
(12.78), \textquotedbl FRIEDMAN\textquotedbl{} (2.21), \textquotedbl HO\textquotedbl{}
(0.015), \textquotedbl MORTGAGE\textquotedbl{} (0.32), and \textquotedbl SN\textquotedbl{}
(0.023) datasets. These results demonstrate the effectiveness of the
RANDOM calculation across a wide range of datasets. In the \textquotedbl FRIEDMAN\textquotedbl{}
dataset, the error value of 2.21 is significantly lower than the corresponding
values of FIXED (2.79) and ADAPT (3.18), underscoring its performance
in this specific dataset. The MIGRANT calculation demonstrates the
best performance in certain datasets, such as \textquotedbl AUTO\textquotedbl{}
(11.46) and \textquotedbl STOCK\textquotedbl{} (4.41), where it outperforms
RANDOM. However, in other datasets, such as \textquotedbl PLASTIC\textquotedbl{}
and \textquotedbl SN,\textquotedbl{} it shows slightly higher error
rates, indicating limitations with specific data. The FIXED calculation
tends to have consistent but not top-performing results, while ADAPT
generally shows higher error values, making it less effective overall.
In summary, the analysis highlights that the RANDOM calculation is
the most reliable and efficient, with the lowest average error and
strong performance across various datasets. However, the MIGRANT calculation
exhibits competitive performance in specific cases, while FIXED and
ADAPT appear to require improvements to rival the other calculations.

Figure \ref{fig:statFClass} evaluates classification datasets for
different differential weight computations within the proposed machine
learning model. The p-values are as follows: RANDOM vs FIXED: p=0.43,
RANDOM vs ADAPT: p=0.024, RANDOM vs MIGRANT: p=0.0021, FIXED vs ADAPT
p=0.12, FIXED vs MIGRANT: p=0.0033 and ADAPT vs MIGRANT: p=0.043.
These results suggest that some comparisons, such as RANDOM vs MIGRANT
and FIXED vs MIGRANT, show strong statistical significance, while
others, such as RANDOM vs FIXED, do not demonstrate significant differences. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{img3}
\par\end{centering}
\caption{Statistical comparison between the proposed variations of the differential
weight mechanisms. The experiments were conducted on the classification
datasets.\label{fig:statFClass}}

\end{figure}

Figure \ref{fig:statFRegression} presents results for regression
datasets using different differential weight computations within the
proposed model. The observed p-values are: RANDOM vs FIXED: p=0.0066,
RANDOM vs ADAPT: p=0.15, RANDOM vs MIGRANT: p=0.66, FIXED vs ADAPT:
p=0.64, FIXED vs MIGRANT: p=0.84 and ADAPT vs MIGRANT: p=0.47. These
findings indicate that most comparisons do not show significant differences,
except for RANDOM vs FIXED, which demonstrates a notable level of
significance. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{img4}
\par\end{centering}
\caption{Statistical comparison for the different variations of the weight
calculation mechanism. The experiments were conducted using the proposed
method on the regression datasets.\label{fig:statFRegression}}

\end{figure}


\subsubsection{Experiment with the number of agents}

An additional experiment was conducted using different values for
the parameter NP, which represents the number of agents. In this experiment
the parameter NP took the values 50, 100 and 200. The experimental
results for the classification datasets are shown in Table \ref{tab:experNPClass}
and for the regression datasets in Table \ref{tab:experNPRegression}.

\begin{table}[H]

\caption{Experimental results for the classification datasets and the proposed
method using a variety of values for the parameter NP.\label{tab:experNPClass}}

\centering{}%
\begin{tabular}{|c|c|c|c|}
\hline 
DATASET & NP=50 & NP=100 & NP=200\tabularnewline
\hline 
\hline 
APPENDICITIS & 14.00\% & 14.00\% & 15.00\%\tabularnewline
\hline 
ALCOHOL & 26.90\% & 25.36\% & 18.33\%\tabularnewline
\hline 
AUSTRALIAN & 30.67\% & 25.83\% & 21.49\%\tabularnewline
\hline 
BALANCE & 7.42\% & 8.13\% & 7.79\%\tabularnewline
\hline 
CLEVELAND & 39.55\% & 42.10\% & 42.38\%\tabularnewline
\hline 
CIRCULAR & 8.50\% & 6.79\% & 6.50\%\tabularnewline
\hline 
DERMATOLOGY & 9.89\% & 11.34\% & 4.97\%\tabularnewline
\hline 
ECOLI & 44.58\% & 41.48\% & 40.30\%\tabularnewline
\hline 
GLASS & 53.24\% & 55.00\% & 54.38\%\tabularnewline
\hline 
HABERMAN & 27.67\% & 27.70\% & 26.53\%\tabularnewline
\hline 
HAYES-ROTH & 36.08\% & 34.69\% & 34.31\%\tabularnewline
\hline 
HEART & 14.89\% & 14.85\% & 13.11\%\tabularnewline
\hline 
HEARTATTACK & 20.73\% & 17.93\% & 21.90\%\tabularnewline
\hline 
HOUSEVOTES & 3.91\% & 5.22\% & 6.09\%\tabularnewline
\hline 
IONOSPHERE & 8.03\% & 10.43\% & 10.37\%\tabularnewline
\hline 
LIVERDISORDER & 32.21\% & 30.18\% & 29.94\%\tabularnewline
\hline 
LYMOGRAPHY & 19.93\% & 18.86\% & 17.93\%\tabularnewline
\hline 
MAMMOGRAPHIC & 15.76\% & 16.53\% & 16.63\%\tabularnewline
\hline 
PARKINSONS & 12.05\% & 11.63\% & 12.79\%\tabularnewline
\hline 
PHONEME & 19.06\% & 19.08\% & 18.10\%\tabularnewline
\hline 
PIMA & 25.75\% & 28.51\% & 25.03\%\tabularnewline
\hline 
POPFAILURES & 5.74\% & 4.07\% & 4.45\%\tabularnewline
\hline 
REGIONS2 & 24.71\% & 24.60\% & 25.19\%\tabularnewline
\hline 
SAHEART & 32.33\% & 31.72\% & 29.26\%\tabularnewline
\hline 
SEGMENT & 46.73\% & 44.27\% & 27.80\%\tabularnewline
\hline 
SONAR & 21.95\% & 22.75\% & 20.50\%\tabularnewline
\hline 
SPIRAL & 43.55\% & 44.08\% & 41.60\%\tabularnewline
\hline 
STATHEART & 18.70\% & 17.67\% & 19.74\%\tabularnewline
\hline 
STUDENT & 3.50\% & 3.70\% & 4.00\%\tabularnewline
\hline 
TRANSFUSION & 23.83\% & 23.87\% & 23.35\%\tabularnewline
\hline 
WDBC & 8.82\% & 7.00\% & 6.73\%\tabularnewline
\hline 
WINE & 8.82\% & 7.12\% & 6.29\%\tabularnewline
\hline 
Z\_F\_S & 8.30\% & 6.73\% & 8.38\%\tabularnewline
\hline 
ZO\_NF\_S & 7.04\% & 6.30\% & 4.32\%\tabularnewline
\hline 
ZONF\_S & 2.08\% & 2.18\% & 1.76\%\tabularnewline
\hline 
ZOO & 6.00\% & 6.00\% & 7.00\%\tabularnewline
\hline 
\textbf{AVERAGE} & \textbf{20.36\%} & \textbf{19.94\%} & \textbf{18.73\%}\tabularnewline
\hline 
\end{tabular}
\end{table}
\begin{table}[H]

\caption{Experimental results for the regression datasets using the proposed
method and a variety of values for parameter NP.\label{tab:experNPRegression}}

\centering{}%
\begin{tabular}{|c|c|c|c|}
\hline 
DATASET & NP=50 & NP=100 & NP=200\tabularnewline
\hline 
\hline 
ABALONE & 4.41 & 4.35 & 4.32\tabularnewline
\hline 
AIRFOIL & 0.003 & 0.003 & 0.002\tabularnewline
\hline 
AUTO & 14.73 & 13.63 & 12.78\tabularnewline
\hline 
BK & 0.019 & 0.018 & 0.02\tabularnewline
\hline 
BL & 0.011 & 0.009 & 0.006\tabularnewline
\hline 
BASEBALL & 63.75 & 57.75 & 60.74\tabularnewline
\hline 
CONCRETE & 0.007 & 0.006 & 0.006\tabularnewline
\hline 
DEE & 0.21 & 0.20 & 0.19\tabularnewline
\hline 
FRIEDMAN & 3.43 & 2.83 & 2.21\tabularnewline
\hline 
FY & 0.047 & 0.041 & 0.067\tabularnewline
\hline 
HO & 0.016 & 0.014 & 0.015\tabularnewline
\hline 
HOUSING & 28.28 & 25.95 & 20.74\tabularnewline
\hline 
LASER & 0.005 & 0.005 & 0.004\tabularnewline
\hline 
LW & 0.01 & 0.012 & 0.011\tabularnewline
\hline 
MORTGAGE & 0.34 & 0.48 & 0.32\tabularnewline
\hline 
PL & 0.022 & 0.022 & 0.022\tabularnewline
\hline 
PLASTIC & 2.16 & 2.16 & 2.16\tabularnewline
\hline 
QUAKE & 0.037 & 0.054 & 0.036\tabularnewline
\hline 
SN & 0.026 & 0.025 & 0.023\tabularnewline
\hline 
STOCK & 5.30 & 6.34 & 5.57\tabularnewline
\hline 
TREASURY & 1.50 & 1.16 & 0.68\tabularnewline
\hline 
\textbf{AVERAGE} & \textbf{5.92} & \textbf{5.48} & \textbf{5.23}\tabularnewline
\hline 
\end{tabular}
\end{table}

The statistical analysis of Table \ref{tab:experNPClass} pertains
to classification datasets, utilizing three different values for the
critical parameter \textquotedbl NP\textquotedbl{} in the proposed
machine learning model: NP=50, NP=100, and NP=200. The computation
with NP=200 demonstrates the lowest average error rate (18.73\%),
indicating the highest efficiency compared to NP=100 (19.94\%) and
NP=50 (20.36\%). This suggests that a higher value of the NP parameter
is generally associated with better performance. In individual datasets,
the computation with NP=200 achieves the lowest error rate in many
cases, such as in \textquotedbl ALCOHOL\textquotedbl{} (18.33\%),
\textquotedbl AUSTRALIAN\textquotedbl{} (21.49\%), \textquotedbl BALANCE\textquotedbl{}
(7.79\%), \textquotedbl DERMATOLOGY\textquotedbl{} (4.97\%), \textquotedbl ECOLI\textquotedbl{}
(40.30\%), and \textquotedbl HEART\textquotedbl{} (13.11\%). In some
of these datasets, the difference between NP=200 and the other two
values is notable. For instance, in the \textquotedbl DERMATOLOGY\textquotedbl{}
dataset, the error rate with NP=200 (4.97\%) is significantly lower
than the corresponding values for NP=50 (9.89\%) and NP=100 (11.34\%),
highlighting the clear superiority of NP=200 for this dataset. However,
there are also datasets where the differences are less pronounced.
For example, in \textquotedbl PHONEME,\textquotedbl{} the error rates
are relatively close across all parameter values, with NP=200 showing
the smallest error (18.10\%). In some other datasets, such as \textquotedbl HOUSEVOTES,\textquotedbl{}
NP=50 has a lower error rate (3.91\%) than the other two parameter
values. This indicates that in certain datasets, increasing the NP
parameter does not necessarily lead to improved performance. Similarly,
in the \textquotedbl Z\_F\_S\textquotedbl{} dataset, NP=100 achieves
the lowest error rate (6.73\%), while NP=200 exhibits a higher rate
(8.38\%), suggesting that performance may also depend on the characteristics
of the data. Despite these exceptions, NP=200 generally exhibits the
best overall performance, achieving the lowest average error rate
and delivering strong results across a wide range of datasets.

The analysis of Table \ref{tab:experNPRegression} focuses on regression
datasets, considering three distinct values for the critical parameter
\textquotedbl NP\textquotedbl{} in the proposed machine learning
model: NP=50, NP=100, and NP=200. The parameter NP=200 achieves the
lowest average error (5.23), making it more effective than NP=100
(5.48) and NP=50 (5.92). This suggests that higher NP values are generally
associated with improved performance. In specific datasets, NP=200
stands out for its superior performance. For instance, in \textquotedbl AIRFOIL\textquotedbl{}
(0.002), \textquotedbl AUTO\textquotedbl{} (12.78), \textquotedbl BL\textquotedbl{}
(0.006), \textquotedbl CONCRETE\textquotedbl{} (0.006), \textquotedbl FRIEDMAN\textquotedbl{}
(2.21), and \textquotedbl TREASURY\textquotedbl{} (0.68), the error
values for NP=200 are the lowest. In the \textquotedbl FRIEDMAN\textquotedbl{}
dataset, NP=200 (2.21) significantly outperforms NP=50 (3.43) and
NP=100 (2.83), demonstrating its effectiveness. However, there are
cases where other NP values show stronger performance. For example,
in the \textquotedbl BK\textquotedbl{} dataset, NP=100 achieves the
lowest error (0.018), while NP=200 (0.02) is slightly worse. Similarly,
in the \textquotedbl FY\textquotedbl{} dataset, NP=100 exhibits the
best performance (0.041), with NP=200 showing a higher error (0.067).
Additionally, in the \textquotedbl BASEBALL\textquotedbl{} dataset,
NP=100 outperforms NP=200, recording an error of 57.75 compared to
60.74. These variations indicate that the effectiveness of the NP
parameter can depend on the characteristics of the dataset. Overall,
NP=200 demonstrates the best average performance, highlighting its
value in most cases. While other NP values achieve lower error rates
in some datasets, NP=200 stands out for its general reliability and
efficiency.

In Figure \ref{fig:statNPClass}, focusing on classification datasets
for different values of the critical parameter \textquotedbl NP\textquotedbl{}
within the proposed model, the p-values are: NP=50 vs NP=100: p=0.17,
NP=50 vs NP=200: p=0.117 and NP=100 vs NP=200: p=0.032. These results
indicate that only the comparison between NP=100 and NP=200 demonstrates
statistical significance. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{img5}
\par\end{centering}
\caption{Statistical comparison for the experiments conducted on the classification
datasets using the proposed method and different values for the parameter
NP.\label{fig:statNPClass}}

\end{figure}

Finally, Figure \ref{fig:statNPRegression} evaluates regression datasets
for different values of the critical parameter \textquotedbl NP\textquotedbl{}
within the proposed model. The respective p-values are: NP=50 vs NP=100:
p=0.08, NP=50 vs NP=200: p=0.012 and NP=100 vs NP=200: p=0.025. These
results show that the comparisons NP=50 vs NP=200 and NP=100 vs NP=200
exhibit statistically significant differences, while the comparison
NP=50 vs NP=100 does not.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.5]{img6}
\par\end{centering}
\caption{Statistical comparison for the conducted experiments on the regression
datasets using the proposed method and different values of the parameter
NP.\label{fig:statNPRegression}}

\end{figure}


\section{Conclusions\label{sec:Conclusions}}

The proposed evolutionary method, based on differential evolution,
proves to be highly effective in optimizing the weights of artificial
neural networks, significantly reducing both training error and overfitting.
Compared to other popular methods such as ADAM, BFGS, genetic algorithms,
and RBF, the new approach demonstrates superior performance in classification
tasks with an average error of 18.73\% and regression tasks with an
average error of 5.23\%. Despite these positive results, certain datasets,
such as \textquotedbl SEGMENT\textquotedbl{} and \textquotedbl STOCK,\textquotedbl{}
exhibit higher errors, likely due to data complexity or inherent noise
that affects prediction accuracy. The random computation of parameter
F and the use of a larger number of agents, such as NP=200, seem to
enhance the system’s performance. However, smaller NP values yield
better results in some cases, indicating the necessity for adaptive
parameter tuning mechanisms during the optimization process. 

Future research could focus on applying the method to more complex
or high-dimensional data, such as images, text, or time series, to
evaluate its scalability and flexibility. Developing mechanisms for
dynamic parameter tuning, such as F, CR, and NP, could eliminate the
need for manual experimentation and enhance overall efficiency. Additionally,
combining this evolutionary approach with other machine learning techniques,
such as reinforcement learning or dynamic neural networks, may improve
generalization and adaptability. Testing the method on different network
architectures, such as recurrent neural networks (RNNs) or graph neural
networks (GNNs), could provide valuable insights into its adaptability
and flexibility across various contexts. Furthermore, a theoretical
analysis of the algorithm’s convergence and stability could offer
a deeper understanding of its strengths and limitations. Finally,
focused studies on datasets where the method underperforms could identify
specific factors, such as noise, class imbalance, or feature complexity,
and propose targeted optimizations to address these challenges effectively.

$ $

\authorcontributions{V.C. and I.G.T. conducted the experiments, employing several datasets
and provided the comparative experiments. D.T. and V.C. performed
the statistical analysis and prepared the manuscript. All authors
have read and agreed to the published version of the manuscript.}

\funding{This research received no external funding.}

\institutionalreview{Not applicable.}

\informedconsent{Not applicable.}

\dataavailability{Not applicable.}

\acknowledgments{This research has been financed by the European Union : Next Generation
EU through the Program Greece 2.0 National Recovery and Resilience
Plan , under the call RESEARCH -- CREATE -- INNOVATE, project name
“iCREW: Intelligent small craft simulator for advanced crew training
using Virtual Reality techniques\textquotedbl{} (project code:TAEDK-06195).}

\conflictsofinterest{The authors declare no conflicts of interest.}

\begin{adjustwidth}{-\extralength}{0cm}{}

\reftitle{References}
\begin{thebibliography}{999}
\bibitem{nn1}Abiodun, O. I., Jantan, A., Omolara, A. E., Dada, K.
V., Mohamed, N. A., \& Arshad, H. (2018). State-of-the-art in artificial
neural network applications: A survey. Heliyon, 4(11).

\bibitem{nn2}Suryadevara, S., \& Yanamala, A. K. Y. (2021). A Comprehensive
Overview of Artificial Neural Networks: Evolution, Architectures,
and Applications. Revista de Inteligencia Artificial en Medicina,
12(1), 51-76.

\bibitem{nn_image}M. Egmont-Petersen, D. de Ridder, H. Handels, Image
processing with neural networks---a review, Pattern Recognition \textbf{35},
pp. 2279-2301, 2002.

\bibitem{nn_timeseries}G.Peter Zhang, Time series forecasting using
a hybrid ARIMA and neural network model, Neurocomputing \textbf{50},
pp. 159-175, 2003.

\bibitem{nn_credit}Z. Huang, H. Chen, C.-Jung Hsu, W.-Hwa Chen, S.
Wu, Credit rating analysis with support vector machines and neural
networks: a market comparative study, Decision Support Systems \textbf{37},
pp. 543-558, 2004.

\bibitem{nnphysics1}P. Baldi, K. Cranmer, T. Faucett et al, Parameterized
neural networks for high-energy physics, Eur. Phys. J. C \textbf{76},
2016.

\bibitem{nnphysics2}Baldi, P., Cranmer, K., Faucett, T., Sadowski,
P., \& Whiteson, D. (2016). Parameterized neural networks for high-energy
physics. The European Physical Journal C, 76(5), 1-7.

\bibitem{nn_solar}A. Kumar Yadav, S.S. Chandel, Solar radiation prediction
using Artificial Neural Network techniques: A review, Renewable and
Sustainable Energy Reviews \textbf{33}, pp. 772-781, 2014.

\bibitem{nnagr2}A. Escamilla-García, G.M. Soto-Zarazúa, M. Toledano-Ayala,
E. Rivas-Araiza, A. Gastélum-Barrios, Abraham,Applications of Artificial
Neural Networks in Greenhouse Technology and Overview for Smart Agriculture
Development, Applied Sciences \textbf{10}, Article number 3835, 2020.

\bibitem{bpnn1}Vora, K., \& Yagnik, S. (2014). A survey on backpropagation
algorithms for feedforward neural networks. International Journal
of Engineering Development and Research, 1(3), 193-197.

\bibitem{bpnn2}K. Vora, S. Yagnik, A survey on backpropagation algorithms
for feedforward neural networks, International Journal of Engineering
Development and Research \textbf{1}, pp. 193-197, 2014.

\bibitem{rpropnn-1}Pajchrowski, T., Zawirski, K., \& Nowopolski,
K. (2014). Neural speed controller trained online by means of modified
RPROP algorithm. IEEE transactions on industrial informatics, 11(2),
560-568.

\bibitem{rpropnn-2}Hermanto, R. P. S., \& Nugroho, A. (2018). Waiting-time
estimation in bank customer queues using RPROP neural networks. Procedia
Computer Science, 135, 35-42.

\bibitem{nn_adam}D. P. Kingma, J. L. Ba, ADAM: a method for stochastic
optimization, in: Proceedings of the 3rd International Conference
on Learning Representations (ICLR 2015), pp. 1--15, 2015.

\bibitem{nn_leve}Lera, G., \& Pinzolas, M. (2002). Neighborhood based
Levenberg-Marquardt algorithm for neural network training. IEEE transactions
on neural networks, 13(5), 1200-1203.

\bibitem{nn_siman}C.L. Kuo, E.E. Kuruoglu, W.K.V. Chan, Neural Network
Structure Optimization by Simulated Annealing, Entropy \textbf{24},
348, 2022.

\bibitem{geneticnn}Reynolds, J., Rezgui, Y., Kwan, A., \& Piriou,
S. (2018). A zone-level, building energy optimisation combining an
artificial neural network, a genetic algorithm, and model predictive
control. Energy, 151, 729-739.

\bibitem{psonn}Das, G., Pattnaik, P. K., \& Padhy, S. K. (2014).
Artificial neural network trained by particle swarm optimization for
non-linear channel equalization. Expert Systems with Applications,
41(7), 3491-3496.

\bibitem{aco_nn}K.M. Salama, A.M. Abdelbar, Learning neural network
structures with ant colony algorithms, Swarm Intell \textbf{9}, pp.
229--265, 2015.

\bibitem{gwo_nn}S. Mirjalili, How effective is the Grey Wolf optimizer
in training multi-layer perceptrons, Appl Intell \textbf{43}, pp.
150--161, 2015.

\bibitem{whale_nn}I. Aljarah, H. Faris, S. Mirjalili, Optimizing
connection weights in neural networks using the whale optimization
algorithm, Soft Comput \textbf{22}, pp. 1--15, 2018. 

\bibitem{tabunn}R.S. Sexton, B. Alidaee, R.E. Dorsey, J.D. Johnson,
Global optimization for artificial neural networks: A tabu search
application. European Journal of Operational Research \textbf{106},
pp. 570-584, 1998.

\bibitem{nn_hybrid}J.-R. Zhang, J. Zhang, T.-M. Lok, M.R. Lyu, A
hybrid particle swarm optimization--back-propagation algorithm for
feedforward neural network training, Applied Mathematics and Computation
\textbf{185}, pp. 1026-1037, 2007.

\bibitem{nn_cascade}G. Zhao, T. Wang, Y. Jin, C. Lang, Y. Li, H.
Ling, The Cascaded Forward algorithm for neural network training,
Pattern Recognition \textbf{161}, 111292, 2025.

\bibitem{nn_gpu1}K-Su Oh, K. Jung, GPU implementation of neural networks,
Pattern Recognition \textbf{37}, pp. 1311-1314, 2004.

\bibitem{nn_gpu2}M. Zhang, K. Hibi, J. Inoue, GPU-accelerated artificial
neural network potential for molecular dynamics simulation, Computer
Physics Communications \textbf{285}, 108655, 2023. 

\bibitem{nn_overfitting}Ying, X. (2019, February). An overview of
overfitting and its solutions. In Journal of physics: Conference series
(Vol. 1168, p. 022022). IOP Publishing.

\bibitem{nnsharing1}S.J. Nowlan and G.E. Hinton, Simplifying neural
networks by soft weight sharing, Neural Computation 4, pp. 473-493,
1992.

\bibitem{nnsharing2}Nowlan, S. J., \& Hinton, G. E. (2018). Simplifying
neural networks by soft weight sharing. In The mathematics of generalization
(pp. 373-394). CRC Press.

\bibitem{nnprunning1}S.J. Hanson and L.Y. Pratt, Comparing biases
for minimal network construction with back propagation, In D.S. Touretzky
(Ed.), Advances in Neural Information Processing Systems, Volume 1,
pp. 177-185, San Mateo, CA: Morgan Kaufmann, 1989.

\bibitem{nnprunning2}M. Augasta and T. Kathirvalavakumar, Pruning
algorithms of neural networks --- a comparative study, Central European
Journal of Computer Science, 2003.

\bibitem{nnearly1}Lutz Prechelt, Automatic early stopping using cross
validation: quantifying the criteria, Neural Networks \textbf{11},
pp. 761-767, 1998.

\bibitem{nnearly2}X. Wu and J. Liu, A New Early Stopping Algorithm
for Improving Neural Network Generalization, 2009 Second International
Conference on Intelligent Computation Technology and Automation, Changsha,
Hunan, 2009, pp. 15-18.

\bibitem{nndecay1}N. K. Treadgold and T. D. Gedeon, Simulated annealing
and weight decay in adaptive learning: the SARPROP algorithm,IEEE
Transactions on Neural Networks \textbf{9}, pp. 662-668, 1998.

\bibitem{nndecay2}M. Carvalho and T. B. Ludermir, Particle Swarm
Optimization of Feed-Forward Neural Networks with Weight Decay, 2006
Sixth International Conference on Hybrid Intelligent Systems (HIS'06),
Rio de Janeiro, Brazil, 2006, pp. 5-5.

\bibitem{nn_arch1}J. Arifovic, R. Gençay, Using genetic algorithms
to select architecture of a feedforward artificial neural network,
Physica A: Statistical Mechanics and its Applications \textbf{289},
pp. 574-594, 2001.

\bibitem{nn_arch2}P.G. Benardos, G.C. Vosniakos, Optimizing feedforward
artificial neural network architecture, Engineering Applications of
Artificial Intelligence \textbf{20}, pp. 365-382, 2007.

\bibitem{nn_arch3}B.A. Garro, R.A. Vázquez, Designing Artificial
Neural Networks Using Particle Swarm Optimization Algorithms, Computational
Intelligence and Neuroscience, 369298, 2015. 

\bibitem{gen_review}Lambora, A., Gupta, K., \& Chopra, K. (2019,
February). Genetic algorithm-A literature review. In 2019 international
conference on machine learning, big data, cloud and parallel computing
(COMITCon) (pp. 380-384). IEEE.

\bibitem{de_review}Pant, M., Zaheer, H., Garcia-Hernandez, L., \&
Abraham, A. (2020). Differential Evolution: A review of more than
two decades of research. Engineering Applications of Artificial Intelligence,
90, 103479.

\bibitem{de_symmetry1}Y.H. Li, J.Q. Wang, X.J. Wang, Y.L. Zhao, X.H.
Lu, D.L. Liu, Community Detection Based on Differential Evolution
Using Social Spider Optimization, Symmetry \textbf{9}, 2017.

\bibitem{de_symmetry3}W. Yang, E.M. Dilanga Siriwardane, R. Dong,
Y. Li, J. Hu, Crystal structure prediction of materials with high
symmetry using differential evolution, J. Phys.: Condens. Matter \textbf{33}
455902, 2021.

\bibitem{de_symmetry6}C.Y. Lee, C.H. Hung, Feature Ranking and Differential
Evolution for Feature Selection in Brushless DC Motor Fault Diagnosis
, Symmetry \textbf{13}, 2021.

\bibitem{de_symmetry7}S. Saha, R. Das, Exploring differential evolution
and particle swarm optimization to develop some symmetry-based automatic
clustering techniques: application to gene clustering, Neural Comput
\& Applic \textbf{30}, pp. 735--757, 2018.

\bibitem{nnc}I.G. Tsoulos, D. Gavrilis, E. Glavas, Neural network
construction and training using grammatical evolution, Neurocomputing
\textbf{72}, pp. 269-277, 2008.

\bibitem{Hornik}Hornik, K., Stinchcombe, M., \& White, H. (1989).
Multilayer feedforward networks are universal approximators. Neural
networks, 2(5), 359-366.

\bibitem{kaelo}P. Kaelo, M.M. Ali, Integrated crossover rules in
real coded genetic algorithms, European Journal of Operational Research
\textbf{176}, pp. 60-76, 2007.

\bibitem[(1989)]{uci} M. Kelly, R. Longjohn, K. Nottingham, The UCI
Machine Learning Repository, https://archive.ics.uci.edu.

\bibitem{Keel}J. Alcalá-Fdez, A. Fernandez, J. Luengo, J. Derrac,
S. García, L. Sánchez, F. Herrera. KEEL Data-Mining Software Tool:
Data Set Repository, Integration of Algorithms and Experimental Analysis
Framework. Journal of Multiple-Valued Logic and Soft Computing 17,
pp. 255-287, 2011.

\bibitem{appendicitis}Weiss, Sholom M. and Kulikowski, Casimir A.,
Computer Systems That Learn: Classification and Prediction Methods
from Statistics, Neural Nets, Machine Learning, and Expert Systems,
Morgan Kaufmann Publishers Inc, 1991.

\bibitem[Tzimourta(2018)]{alcohol}Tzimourta, K.D.; Tsoulos, I.; Bilero,
I.T.; Tzallas, A.T.; Tsipouras, M.G.; Giannakeas, N. Direct Assessment
of Alcohol Consumption in Mental State Using Brain Computer Interfaces
and Grammatical Evolution. Inventions 2018, 3, 51.

\bibitem[Quinlan(2018)]{australian}J.R. Quinlan, Simplifying Decision
Trees. International Journal of Man-Machine Studies \textbf{27}, pp.
221-234, 1987. 

\bibitem{balance}T. Shultz, D. Mareschal, W. Schmidt, Modeling Cognitive
Development on Balance Scale Phenomena, Machine Learning \textbf{16},
pp. 59-88, 1994.

\bibitem[(2004)]{cleveland1}Z.H. Zhou,Y. Jiang, NeC4.5: neural ensemble
based C4.5,\textquotedbl{} in IEEE Transactions on Knowledge and Data
Engineering \textbf{16}, pp. 770-773, 2004.

\bibitem{cleveland2}R. Setiono , W.K. Leow, FERNN: An Algorithm for
Fast Extraction of Rules from Neural Networks, Applied Intelligence
\textbf{12}, pp. 15-25, 2000.

\bibitem[(1998)]{dermatology}G. Demiroz, H.A. Govenir, N. Ilter,
Learning Differential Diagnosis of Eryhemato-Squamous Diseases using
Voting Feature Intervals, Artificial Intelligence in Medicine. \textbf{13},
pp. 147--165, 1998.

\bibitem[(1996)]{ecoli}P. Horton, K.Nakai, A Probabilistic Classification
System for Predicting the Cellular Localization Sites of Proteins,
In: Proceedings of International Conference on Intelligent Systems
for Molecular Biology \textbf{4}, pp. 109-15, 1996.

\bibitem[(1977)]{hayes-roth}B. Hayes-Roth, B., F. Hayes-Roth. Concept
learning and the recognition and classification of exemplars. Journal
of Verbal Learning and Verbal Behavior \textbf{16}, pp. 321-338, 1977.

\bibitem[(1997)]{heart}I. Kononenko, E. Šimec, M. Robnik-Šikonja,
Overcoming the Myopia of Inductive Learning Algorithms with RELIEFF,
Applied Intelligence \textbf{7}, pp. 39--55, 1997

\bibitem[(2002)]{housevotes}R.M. French, N. Chater, Using noise to
compute error surfaces in connectionist networks: a novel means of
reducing catastrophic forgetting, Neural Comput. \textbf{14}, pp.
1755-1769, 2002.

\bibitem[(2004)]{ion1}J.G. Dy , C.E. Brodley, Feature Selection for
Unsupervised Learning, The Journal of Machine Learning Research \textbf{5},
pp 845--889, 2004.

\bibitem{ion2}S. J. Perantonis, V. Virvilis, Input Feature Extraction
for Multilayered Perceptrons Using Supervised Principal Component
Analysis, Neural Processing Letters \textbf{10}, pp 243--252, 1999.

\bibitem[(2002)]{liver} J. Garcke, M. Griebel, Classification with
sparse grids using simplicial basis functions, Intell. Data Anal.
\textbf{6}, pp. 483-502, 2002.

\bibitem{liver1}J. Mcdermott, R.S. Forsyth, Diagnosing a disorder
in a classification benchmark, Pattern Recognition Letters \textbf{73},
pp. 41-43, 2016.

\bibitem[(2002)]{lymography}G. Cestnik, I. Konenenko, I. Bratko,
Assistant-86: A Knowledge-Elicitation Tool for Sophisticated Users.
In: Bratko, I. and Lavrac, N., Eds., Progress in Machine Learning,
Sigma Press, Wilmslow, pp. 31-45, 1987. 

\bibitem[(2007)]{mammographic}M. Elter, R. Schulz-Wendtland, T. Wittenberg,
The prediction of breast cancer biopsy outcomes using two CAD approaches
that both emphasize an intelligible decision process, Med Phys. \textbf{34},
pp. 4164-72, 2007.

\bibitem[(2007)]{parkinsons1}M.A. Little, P.E. McSharry, S.J Roberts
et al, Exploiting Nonlinear Recurrence and Fractal Scaling Properties
for Voice Disorder Detection. BioMed Eng OnLine \textbf{6}, 23, 2007.

\bibitem{parkinsons2}M.A. Little, P.E. McSharry, E.J. Hunter, J.
Spielman, L.O. Ramig, Suitability of dysphonia measurements for telemonitoring
of Parkinson's disease. IEEE Trans Biomed Eng. \textbf{56}, pp. 1015-1022,
2009.

\bibitem[(2007)]{pima}J.W. Smith, J.E. Everhart, W.C. Dickson, W.C.
Knowler, R.S. Johannes, Using the ADAP learning algorithm to forecast
the onset of diabetes mellitus, In: Proceedings of the Symposium on
Computer Applications and Medical Care IEEE Computer Society Press,
pp.261-265, 1988.

\bibitem[(2007)]{popfailures}D.D. Lucas, R. Klein, J. Tannahill,
D. Ivanova, S. Brandon, D. Domyancic, Y. Zhang, Failure analysis of
parameter-induced simulation crashes in climate models, Geoscientific
Model Development \textbf{6}, pp. 1157-1171, 2013.

\bibitem[(2007)]{regions2}N. Giannakeas, M.G. Tsipouras, A.T. Tzallas,
K. Kyriakidi, Z.E. Tsianou, P. Manousou, A. Hall, E.C. Karvounis,
V. Tsianos, E. Tsianos, A clustering based method for collagen proportional
area extraction in liver biopsy images (2015) Proceedings of the Annual
International Conference of the IEEE Engineering in Medicine and Biology
Society, EMBS, 2015-November, art. no. 7319047, pp. 3097-3100. 

\bibitem[(2007)]{saheart}T. Hastie, R. Tibshirani, Non-parametric
logistic and proportional odds regression, JRSS-C (Applied Statistics)
\textbf{36}, pp. 260--276, 1987.

\bibitem{segment}M. Dash, H. Liu, P. Scheuermann, K. L. Tan, Fast
hierarchical clustering and its validation, Data \& Knowledge Engineering
\textbf{44}, pp 109--138, 2003.

\bibitem[(2007)]{student}P. Cortez, A. M. Gonçalves Silva, Using
data mining to predict secondary school student performance, In Proceedings
of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) (pp. 5--12).
EUROSIS-ETI, 2008.

\bibitem[(2007)]{transfusion}I-Cheng Yeh, King-Jang Yang, Tao-Ming
Ting, Knowledge discovery on RFM model using Bernoulli sequence, Expert
Systems with Applications \textbf{36}, pp. 5866-5871, 2009.

\bibitem[(2007)]{wdbc1}Jeyasingh, S., \& Veluchamy, M. (2017). Modified
bat algorithm for feature selection with the wisconsin diagnosis breast
cancer (WDBC) dataset. Asian Pacific journal of cancer prevention:
APJCP, 18(5), 1257.

\bibitem[(2007)]{wdbc2}Alshayeji, M. H., Ellethy, H., \& Gupta, R.
(2022). Computer-aided detection of breast cancer on the Wisconsin
dataset: An artificial neural networks approach. Biomedical signal
processing and control, 71, 103141.

\bibitem[(2007)]{wine1}M. Raymer, T.E. Doom, L.A. Kuhn, W.F. Punch,
Knowledge discovery in medical and biological datasets using a hybrid
Bayes classifier/evolutionary algorithm. IEEE transactions on systems,
man, and cybernetics. Part B, Cybernetics : a publication of the IEEE
Systems, Man, and Cybernetics Society, \textbf{33} , pp. 802-813,
2003.

\bibitem{wine2}P. Zhong, M. Fukushima, Regularized nonsmooth Newton
method for multi-class support vector machines, Optimization Methods
and Software \textbf{22}, pp. 225-236, 2007.

\bibitem[(2007)]{eeg1}R. G. Andrzejak, K. Lehnertz, F.Mormann, C.
Rieke, P. David, and C. E. Elger, “Indications of nonlinear deterministic
and finite-dimensional structures in time series of brain electrical
activity: dependence on recording region and brain state,” Physical
Review E, vol. 64, no. 6, Article ID 061907, 8 pages, 2001. 

\bibitem{eeg2}A. T. Tzallas, M. G. Tsipouras, and D. I. Fotiadis,
“Automatic Seizure Detection Based on Time-Frequency Analysis and
Artificial Neural Networks,” Computational Intelligence and Neuroscience,
vol. 2007, Article ID 80510, 13 pages, 2007. doi:10.1155/2007/80510

\bibitem[(2007)]{zoo}M. Koivisto, K. Sood, Exact Bayesian Structure
Discovery in Bayesian Networks, The Journal of Machine Learning Research\textbf{
5}, pp. 549--573, 2004.

\bibitem[(2007)]{abalone}Nash, W.J.; Sellers, T.L.; Talbot, S.R.;
Cawthor, A.J.; Ford, W.B. The Population Biology of Abalone (\_Haliotis\_
species) in Tasmania. I. Blacklip Abalone (\_H. rubra\_) from the
North Coast and Islands of Bass Strait, Sea Fisheries Division; Technical
Report No. 48; Department of Primary Industry and Fisheries, Tasmania:
Hobart, Australia, 1994; ISSN 1034-3288

\bibitem[(2007)]{airfoil}Brooks, T.F.; Pope, D.S.; Marcolini, A.M.
Airfoil Self-Noise and Prediction. Technical Report, NASA RP-1218.
July 1989. Available online: https://ntrs.nasa.gov/citations/19890016302
(accessed on 14 November 2024).

\bibitem[(2007)]{concrete}I.Cheng Yeh, Modeling of strength of high
performance concrete using artificial neural networks, Cement and
Concrete Research. \textbf{28}, pp. 1797-1808, 1998. 

\bibitem{friedman}Friedman, J. (1991): Multivariate Adaptative Regression
Splines. Annals of Statistics, 19:1, 1-{}-141. 

\bibitem[(2007)]{housing}D. Harrison and D.L. Rubinfeld, Hedonic
prices and the demand for clean ai, J. Environ. Economics \& Management
\textbf{5}, pp. 81-102, 1978.

\bibitem{powell}M.J.D Powell, A Tolerant Algorithm for Linearly Constrained
Optimization Calculations, Mathematical Programming \textbf{45}, pp.
547-566, 1989. 

\bibitem{neat}K. O. Stanley, R. Miikkulainen, Evolving Neural Networks
through Augmenting Topologies, Evolutionary Computation \textbf{10},
pp. 99-127, 2002.

\bibitem[(1991)]{rbf1}J. Park and I. W. Sandberg, Universal Approximation
Using Radial-Basis-Function Networks, Neural Computation \textbf{3},
pp. 246-257, 1991.

\bibitem{rbf2}G.A. Montazer, D. Giveki, M. Karami, H. Rastegar, Radial
basis function neural networks: A review. Comput. Rev. J \textbf{1},
pp. 52-74, 2018.

\bibitem{de_migrant}J. Cheng, G. Zhang, F. Neri, Enhancing distributed
differential evolution with multicultural migration for global numerical
optimization. Information Sciences \textbf{247}, pp. 72-93, 2013.

\bibitem{de_adaptive}Wu, K., Liu, Z., Ma, N., \& Wang, D. (2022).
A Dynamic Adaptive Weighted Differential Evolutionary Algorithm. Computational
Intelligence and Neuroscience, 2022(1), 1318044.

\end{thebibliography}

\end{adjustwidth}{}
\end{document}
